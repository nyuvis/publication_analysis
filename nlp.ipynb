{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./IEEE VIS papers 1990-2018 - Main dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the use of critical point analysis to generate representations of the vector field topology of numerical flow data sets is discussed. critical points are located and characterized in a two-dimensional domain, which may be either a two-dimensional flow field or the tangential velocity field near a three-dimensional body. tangent curves are then integrated out along the principal directions of certain classes of critical points. the points and curves are linked to form a skeleton representing the two-dimensional vector field topology. when generated from the tangential velocity field near a body in a three-dimensional flow, the skeleton includes the critical points and curves which provide a basis for analyzing the three-dimensional structure of the flow separation.&lt;&lt;etx&gt;&gt;'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Abstract'] = [abstract.lower() for abstract in df['Abstract']]\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the use of critical point analysis to generate representations of the vector field topology of numerical flow data sets is discussed. critical points are located and characterized in a two-dimensional domain, which may be either a two-dimensional flow field or the tangential velocity field near a three-dimensional body. tangent curves are then integrated out along the principal directions of certain classes of critical points. the points and curves are linked to form a skeleton representing the two-dimensional vector field topology. when generated from the tangential velocity field near a body in a three-dimensional flow, the skeleton includes the critical points and curves which provide a basis for analyzing the three-dimensional structure of the flow separation.'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Abstract'] = [abstract[:-19] for abstract in df['Abstract']]\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the use of critical point analysis to generate representations of the vector field topology of numerical flow data sets is discussed critical points are located and characterized in a twodimensional domain which may be either a twodimensional flow field or the tangential velocity field near a threedimensional body tangent curves are then integrated out along the principal directions of certain classes of critical points the points and curves are linked to form a skeleton representing the twodimensional vector field topology when generated from the tangential velocity field near a body in a threedimensional flow the skeleton includes the critical points and curves which provide a basis for analyzing the threedimensional structure of the flow separation'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'analysis',\n",
       " 'to',\n",
       " 'generate',\n",
       " 'representations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'of',\n",
       " 'numerical',\n",
       " 'flow',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'is',\n",
       " 'discussed',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'are',\n",
       " 'located',\n",
       " 'and',\n",
       " 'characterized',\n",
       " 'in',\n",
       " 'a',\n",
       " 'twodimensional',\n",
       " 'domain',\n",
       " 'which',\n",
       " 'may',\n",
       " 'be',\n",
       " 'either',\n",
       " 'a',\n",
       " 'twodimensional',\n",
       " 'flow',\n",
       " 'field',\n",
       " 'or',\n",
       " 'the',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'a',\n",
       " 'threedimensional',\n",
       " 'body',\n",
       " 'tangent',\n",
       " 'curves',\n",
       " 'are',\n",
       " 'then',\n",
       " 'integrated',\n",
       " 'out',\n",
       " 'along',\n",
       " 'the',\n",
       " 'principal',\n",
       " 'directions',\n",
       " 'of',\n",
       " 'certain',\n",
       " 'classes',\n",
       " 'of',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'the',\n",
       " 'points',\n",
       " 'and',\n",
       " 'curves',\n",
       " 'are',\n",
       " 'linked',\n",
       " 'to',\n",
       " 'form',\n",
       " 'a',\n",
       " 'skeleton',\n",
       " 'representing',\n",
       " 'the',\n",
       " 'twodimensional',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'when',\n",
       " 'generated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'a',\n",
       " 'body',\n",
       " 'in',\n",
       " 'a',\n",
       " 'threedimensional',\n",
       " 'flow',\n",
       " 'the',\n",
       " 'skeleton',\n",
       " 'includes',\n",
       " 'the',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'and',\n",
       " 'curves',\n",
       " 'which',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'analyzing',\n",
       " 'the',\n",
       " 'threedimensional',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'flow',\n",
       " 'separation']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    return tokens\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: tokenize(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sydney\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'analysis',\n",
       " 'generate',\n",
       " 'representations',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'numerical',\n",
       " 'flow',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'discussed',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'located',\n",
       " 'characterized',\n",
       " 'two',\n",
       " 'dimensional',\n",
       " 'domain',\n",
       " 'may',\n",
       " 'either',\n",
       " 'two',\n",
       " 'dimensional',\n",
       " 'flow',\n",
       " 'field',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'three',\n",
       " 'dimensional',\n",
       " 'body',\n",
       " 'tangent',\n",
       " 'curves',\n",
       " 'integrated',\n",
       " 'along',\n",
       " 'principal',\n",
       " 'directions',\n",
       " 'certain',\n",
       " 'classes',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'points',\n",
       " 'curves',\n",
       " 'linked',\n",
       " 'form',\n",
       " 'skeleton',\n",
       " 'representing',\n",
       " 'two',\n",
       " 'dimensional',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'generated',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'body',\n",
       " 'three',\n",
       " 'dimensional',\n",
       " 'flow',\n",
       " 'skeleton',\n",
       " 'includes',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'curves',\n",
       " 'provide',\n",
       " 'basis',\n",
       " 'analyzing',\n",
       " 'three',\n",
       " 'dimensional',\n",
       " 'structure',\n",
       " 'flow',\n",
       " 'separation',\n",
       " '']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sydney\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'analysis',\n",
       " 'to',\n",
       " 'generate',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'of',\n",
       " 'numerical',\n",
       " 'flow',\n",
       " 'data',\n",
       " 'set',\n",
       " 'is',\n",
       " 'discussed',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'are',\n",
       " 'located',\n",
       " 'and',\n",
       " 'characterized',\n",
       " 'in',\n",
       " 'a',\n",
       " 'twodimensional',\n",
       " 'domain',\n",
       " 'which',\n",
       " 'may',\n",
       " 'be',\n",
       " 'either',\n",
       " 'a',\n",
       " 'twodimensional',\n",
       " 'flow',\n",
       " 'field',\n",
       " 'or',\n",
       " 'the',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'a',\n",
       " 'threedimensional',\n",
       " 'body',\n",
       " 'tangent',\n",
       " 'curve',\n",
       " 'are',\n",
       " 'then',\n",
       " 'integrated',\n",
       " 'out',\n",
       " 'along',\n",
       " 'the',\n",
       " 'principal',\n",
       " 'direction',\n",
       " 'of',\n",
       " 'certain',\n",
       " 'class',\n",
       " 'of',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'the',\n",
       " 'point',\n",
       " 'and',\n",
       " 'curve',\n",
       " 'are',\n",
       " 'linked',\n",
       " 'to',\n",
       " 'form',\n",
       " 'a',\n",
       " 'skeleton',\n",
       " 'representing',\n",
       " 'the',\n",
       " 'twodimensional',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'when',\n",
       " 'generated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'a',\n",
       " 'body',\n",
       " 'in',\n",
       " 'a',\n",
       " 'threedimensional',\n",
       " 'flow',\n",
       " 'the',\n",
       " 'skeleton',\n",
       " 'includes',\n",
       " 'the',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'and',\n",
       " 'curve',\n",
       " 'which',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'analyzing',\n",
       " 'the',\n",
       " 'threedimensional',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'flow',\n",
       " 'separation']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       [the, use, of, critical, point, analysis, to, ...\n",
       "2       [the, author, discus, fast, flow, analysis, so...\n",
       "3       [the, vis5d, system, provides, highly, interac...\n",
       "4       [the, author, present, a, simple, procedural, ...\n",
       "5       [some, idea, and, technique, for, visualizing,...\n",
       "                              ...                        \n",
       "3103    [completing, text, analysis, task, is, a, cont...\n",
       "3104    [social, data, chart, visual, presentation, of...\n",
       "3105    [constructing, latent, vector, representation,...\n",
       "3106    [we, present, smartexplore, a, novel, visual, ...\n",
       "3107    [deep, neural, network, dnns, are, vulnerable,...\n",
       "Name: Abstract, Length: 3035, dtype: object"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-310-92076204fea5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtfidf_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\sydney\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \"\"\"\n\u001b[0;32m   1839\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sydney\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sydney\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sydney\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sydney\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf_df = tfidf.fit_transform(df['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = cluster_model.fit(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
