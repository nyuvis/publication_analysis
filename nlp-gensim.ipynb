{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.sklearn_api import TfIdfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./IEEE VIS papers 1990-2018 - Main dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the use of critical point analysis to generate representations of the vector field topology of numerical flow data sets is discussed. critical points are located and characterized in a two-dimensional domain, which may be either a two-dimensional flow field or the tangential velocity field near a three-dimensional body. tangent curves are then integrated out along the principal directions of certain classes of critical points. the points and curves are linked to form a skeleton representing the two-dimensional vector field topology. when generated from the tangential velocity field near a body in a three-dimensional flow, the skeleton includes the critical points and curves which provide a basis for analyzing the three-dimensional structure of the flow separation.&lt;&lt;etx&gt;&gt;'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Abstract'] = [abstract.lower() for abstract in df['Abstract']]\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the use of critical point analysis to generate representations of the vector field topology of numerical flow data sets is discussed. critical points are located and characterized in a two-dimensional domain, which may be either a two-dimensional flow field or the tangential velocity field near a three-dimensional body. tangent curves are then integrated out along the principal directions of certain classes of critical points. the points and curves are linked to form a skeleton representing the two-dimensional vector field topology. when generated from the tangential velocity field near a body in a three-dimensional flow, the skeleton includes the critical points and curves which provide a basis for analyzing the three-dimensional structure of the flow separation.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Abstract'] = [abstract[:-19] for abstract in df['Abstract']]\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the use of critical point analysis to generate representations of the vector field topology of numerical flow data sets is discussed critical points are located and characterized in a twodimensional domain which may be either a twodimensional flow field or the tangential velocity field near a threedimensional body tangent curves are then integrated out along the principal directions of certain classes of critical points the points and curves are linked to form a skeleton representing the twodimensional vector field topology when generated from the tangential velocity field near a body in a threedimensional flow the skeleton includes the critical points and curves which provide a basis for analyzing the threedimensional structure of the flow separation'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'analysis',\n",
       " 'to',\n",
       " 'generate',\n",
       " 'representations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'of',\n",
       " 'numerical',\n",
       " 'flow',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'is',\n",
       " 'discussed',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'are',\n",
       " 'located',\n",
       " 'and',\n",
       " 'characterized',\n",
       " 'in',\n",
       " 'a',\n",
       " 'twodimensional',\n",
       " 'domain',\n",
       " 'which',\n",
       " 'may',\n",
       " 'be',\n",
       " 'either',\n",
       " 'a',\n",
       " 'twodimensional',\n",
       " 'flow',\n",
       " 'field',\n",
       " 'or',\n",
       " 'the',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'a',\n",
       " 'threedimensional',\n",
       " 'body',\n",
       " 'tangent',\n",
       " 'curves',\n",
       " 'are',\n",
       " 'then',\n",
       " 'integrated',\n",
       " 'out',\n",
       " 'along',\n",
       " 'the',\n",
       " 'principal',\n",
       " 'directions',\n",
       " 'of',\n",
       " 'certain',\n",
       " 'classes',\n",
       " 'of',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'the',\n",
       " 'points',\n",
       " 'and',\n",
       " 'curves',\n",
       " 'are',\n",
       " 'linked',\n",
       " 'to',\n",
       " 'form',\n",
       " 'a',\n",
       " 'skeleton',\n",
       " 'representing',\n",
       " 'the',\n",
       " 'twodimensional',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'when',\n",
       " 'generated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'a',\n",
       " 'body',\n",
       " 'in',\n",
       " 'a',\n",
       " 'threedimensional',\n",
       " 'flow',\n",
       " 'the',\n",
       " 'skeleton',\n",
       " 'includes',\n",
       " 'the',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'and',\n",
       " 'curves',\n",
       " 'which',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'analyzing',\n",
       " 'the',\n",
       " 'threedimensional',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'flow',\n",
       " 'separation']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    return tokens\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: tokenize(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junyuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'analysis',\n",
       " 'generate',\n",
       " 'representations',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'numerical',\n",
       " 'flow',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'discussed',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'located',\n",
       " 'characterized',\n",
       " 'twodimensional',\n",
       " 'domain',\n",
       " 'may',\n",
       " 'either',\n",
       " 'twodimensional',\n",
       " 'flow',\n",
       " 'field',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'threedimensional',\n",
       " 'body',\n",
       " 'tangent',\n",
       " 'curves',\n",
       " 'integrated',\n",
       " 'along',\n",
       " 'principal',\n",
       " 'directions',\n",
       " 'certain',\n",
       " 'classes',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'points',\n",
       " 'curves',\n",
       " 'linked',\n",
       " 'form',\n",
       " 'skeleton',\n",
       " 'representing',\n",
       " 'twodimensional',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'generated',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'body',\n",
       " 'threedimensional',\n",
       " 'flow',\n",
       " 'skeleton',\n",
       " 'includes',\n",
       " 'critical',\n",
       " 'points',\n",
       " 'curves',\n",
       " 'provide',\n",
       " 'basis',\n",
       " 'analyzing',\n",
       " 'threedimensional',\n",
       " 'structure',\n",
       " 'flow',\n",
       " 'separation']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/junyuan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'analysis',\n",
       " 'generate',\n",
       " 'representation',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'numerical',\n",
       " 'flow',\n",
       " 'data',\n",
       " 'set',\n",
       " 'discussed',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'located',\n",
       " 'characterized',\n",
       " 'twodimensional',\n",
       " 'domain',\n",
       " 'may',\n",
       " 'either',\n",
       " 'twodimensional',\n",
       " 'flow',\n",
       " 'field',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'threedimensional',\n",
       " 'body',\n",
       " 'tangent',\n",
       " 'curve',\n",
       " 'integrated',\n",
       " 'along',\n",
       " 'principal',\n",
       " 'direction',\n",
       " 'certain',\n",
       " 'class',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'point',\n",
       " 'curve',\n",
       " 'linked',\n",
       " 'form',\n",
       " 'skeleton',\n",
       " 'representing',\n",
       " 'twodimensional',\n",
       " 'vector',\n",
       " 'field',\n",
       " 'topology',\n",
       " 'generated',\n",
       " 'tangential',\n",
       " 'velocity',\n",
       " 'field',\n",
       " 'near',\n",
       " 'body',\n",
       " 'threedimensional',\n",
       " 'flow',\n",
       " 'skeleton',\n",
       " 'includes',\n",
       " 'critical',\n",
       " 'point',\n",
       " 'curve',\n",
       " 'provide',\n",
       " 'basis',\n",
       " 'analyzing',\n",
       " 'threedimensional',\n",
       " 'structure',\n",
       " 'flow',\n",
       " 'separation']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       [use, critical, point, analysis, generate, rep...\n",
       "2       [author, discus, fast, flow, analysis, softwar...\n",
       "3       [vis5d, system, provides, highly, interactive,...\n",
       "4       [author, present, simple, procedural, interfac...\n",
       "5       [idea, technique, visualizing, volumetric, dat...\n",
       "                              ...                        \n",
       "3103    [completing, text, analysis, task, continuous,...\n",
       "3104    [social, data, chart, visual, presentation, qu...\n",
       "3105    [constructing, latent, vector, representation,...\n",
       "3106    [present, smartexplore, novel, visual, analyti...\n",
       "3107    [deep, neural, network, dnns, vulnerable, mali...\n",
       "Name: Abstract, Length: 3035, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['Abstract'])\n",
    "\n",
    "model = TfIdfTransformer(dictionary=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1),\n",
       " (24, 1),\n",
       " (49, 1),\n",
       " (56, 1),\n",
       " (119, 3),\n",
       " (125, 2),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (152, 1),\n",
       " (168, 2),\n",
       " (181, 2),\n",
       " (201, 1),\n",
       " (209, 1),\n",
       " (334, 2),\n",
       " (341, 1),\n",
       " (342, 1),\n",
       " (343, 1),\n",
       " (344, 1),\n",
       " (345, 1),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (348, 1),\n",
       " (349, 1),\n",
       " (350, 1),\n",
       " (351, 2),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 1),\n",
       " (357, 1),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 1),\n",
       " (362, 1),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 1),\n",
       " (368, 1),\n",
       " (369, 1),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 1),\n",
       " (374, 2),\n",
       " (375, 3),\n",
       " (376, 1),\n",
       " (377, 2),\n",
       " (378, 1),\n",
       " (379, 3),\n",
       " (380, 1),\n",
       " (381, 2),\n",
       " (382, 1),\n",
       " (383, 1),\n",
       " (384, 1),\n",
       " (385, 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word.doc2bow(df['Abstract'].iloc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in df['Abstract']]\n",
    "\n",
    "num_docs = id2word.num_docs\n",
    "num_terms = len(id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'along', 1], [1, 'analysis', 1], [2, 'analyzing', 1], [3, 'basis', 1], [4, 'body', 2], [5, 'certain', 1], [6, 'characterized', 1], [7, 'class', 1], [8, 'critical', 4], [9, 'curve', 3], [10, 'data', 1], [11, 'direction', 1], [12, 'discussed', 1], [13, 'domain', 1], [14, 'either', 1], [15, 'field', 5], [16, 'flow', 4], [17, 'form', 1], [18, 'generate', 1], [19, 'generated', 1], [20, 'includes', 1], [21, 'integrated', 1], [22, 'linked', 1], [23, 'located', 1], [24, 'may', 1], [25, 'near', 2], [26, 'numerical', 1], [27, 'point', 5], [28, 'principal', 1], [29, 'provide', 1], [30, 'representation', 1], [31, 'representing', 1], [32, 'separation', 1], [33, 'set', 1], [34, 'skeleton', 2], [35, 'structure', 1], [36, 'tangent', 1], [37, 'tangential', 2], [38, 'threedimensional', 3], [39, 'topology', 2], [40, 'twodimensional', 3], [41, 'use', 1], [42, 'vector', 2], [43, 'velocity', 2]]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus[:1]:\n",
    "    print([[id, id2word[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = model.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an array of tf-idf vectors\n",
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "\n",
    "corpus_tfidf_dense = corpus2dense(tfidf_corpus, num_terms, num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16319, 3035)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus_tfidf_dense.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = cluster_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    explain the cluster \n",
    "1. generate the avg tfidf for different clusters\n",
    "2. what are the first 10,20 most important words that are used in the abstracts of this cluster\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
